{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df228b87b8242b99a9df121e2e9ca39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/media/yanncauchepin/ExternalDisk/Datasets/ComputerVisionImages/digit_recognizer/train.csv')\n",
    "df_test = pd.read_csv('/media/yanncauchepin/ExternalDisk/Datasets/ComputerVisionImages/digit_recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count\n",
      "label       \n",
      "1       4684\n",
      "7       4401\n",
      "3       4351\n",
      "9       4188\n",
      "2       4177\n",
      "6       4137\n",
      "0       4132\n",
      "4       4072\n",
      "8       4063\n",
      "5       3795\n"
     ]
    }
   ],
   "source": [
    "distinct_labels = df_train['label'].value_counts()\n",
    "print(pd.DataFrame(distinct_labels))\n",
    "classes = len(df_train[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "        'Value': [f1, precision, recall]\n",
    "    })\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, columns=df_train[\"label\"].unique(), index=df_train[\"label\"].unique())\n",
    "    \n",
    "    return metrics_df, cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(df_train['label'])\n",
    "X_train = np.array(df_train.drop('label', axis=1))\n",
    "X_test = np.array(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train / 255.0\n",
    "X_valid_norm = X_valid / 255.0\n",
    "X_test_norm = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.962301\n",
      "1  Precision  0.963080\n",
      "2     Recall  0.962381,      1    0    4    7    3    5    8    9    2    6\n",
      "1  817    0    1    0    0    4    4    0    0    1\n",
      "0    0  934    1    0    0    0    0    2    0    0\n",
      "4    9   17  789    3    1    2    0   13    0    1\n",
      "7    2    6    5  831    0   14    0    5    3    4\n",
      "3    0   10    0    0  783    0    1    1    0   19\n",
      "5    4    5    0   10    0  720    7    0    3   10\n",
      "8    5    2    0    0    1    4  815    0    0    0\n",
      "9    0   10    0    0    1    0    0  862    0    7\n",
      "2    9   15    2   15    3   18    3    3  732   13\n",
      "6    4    1    1    3   10    3    0   15    0  801)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=classes)\n",
    "knn_classifier.fit(X_train_norm, y_train)\n",
    "\n",
    "y_pred = knn_classifier.predict(X_valid_norm)\n",
    "\n",
    "knn_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(knn_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(      Metric     Value\n",
      "0   F1 Score  0.975350\n",
      "1  Precision  0.975385\n",
      "2     Recall  0.975357,      1    0    4    7    3    5    8    9    2    6\n",
      "1  812    0    1    0    2    1    6    0    2    3\n",
      "0    0  930    2    3    0    0    0    0    2    0\n",
      "4    0    0  815    4    5    0    0    8    2    1\n",
      "7    3    2    7  832    3   10    0    3    3    7\n",
      "3    1    4    0    0  794    0    2    1    1   11\n",
      "5    4    1    1    7    1  732    6    0    4    3\n",
      "8    0    1    1    0    0    5  817    0    3    0\n",
      "9    0    3    4    2    2    1    0  861    1    6\n",
      "2    3    1    0    2    0    6    3    1  788    9\n",
      "6    3    2    0    3    4    2    0    9    3  812)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.1,  eval_metric='logloss')\n",
    "xgb_classifier.fit(X_train_norm, y_train, verbose=1)\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_valid_norm)\n",
    "\n",
    "xgb_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(xgb_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.0962791\ttotal: 397ms\tremaining: 6m 36s\n",
      "2:\tlearn: 1.8310850\ttotal: 907ms\tremaining: 5m 1s\n",
      "4:\tlearn: 1.6302713\ttotal: 1.45s\tremaining: 4m 48s\n",
      "6:\tlearn: 1.4751555\ttotal: 1.96s\tremaining: 4m 37s\n",
      "8:\tlearn: 1.3460221\ttotal: 2.44s\tremaining: 4m 28s\n",
      "10:\tlearn: 1.2434894\ttotal: 2.94s\tremaining: 4m 24s\n",
      "12:\tlearn: 1.1528075\ttotal: 3.46s\tremaining: 4m 22s\n",
      "14:\tlearn: 1.0777297\ttotal: 3.91s\tremaining: 4m 17s\n",
      "16:\tlearn: 1.0118651\ttotal: 4.4s\tremaining: 4m 14s\n",
      "18:\tlearn: 0.9546359\ttotal: 4.84s\tremaining: 4m 9s\n",
      "20:\tlearn: 0.8999357\ttotal: 5.3s\tremaining: 4m 7s\n",
      "22:\tlearn: 0.8518069\ttotal: 5.8s\tremaining: 4m 6s\n",
      "24:\tlearn: 0.8097365\ttotal: 6.33s\tremaining: 4m 6s\n",
      "26:\tlearn: 0.7709663\ttotal: 6.89s\tremaining: 4m 8s\n",
      "28:\tlearn: 0.7421569\ttotal: 7.36s\tremaining: 4m 6s\n",
      "30:\tlearn: 0.7083525\ttotal: 7.89s\tremaining: 4m 6s\n",
      "32:\tlearn: 0.6796631\ttotal: 8.36s\tremaining: 4m 5s\n",
      "34:\tlearn: 0.6590664\ttotal: 8.73s\tremaining: 4m\n",
      "36:\tlearn: 0.6324410\ttotal: 9.23s\tremaining: 4m\n",
      "38:\tlearn: 0.6080263\ttotal: 9.72s\tremaining: 3m 59s\n",
      "40:\tlearn: 0.5898589\ttotal: 10.1s\tremaining: 3m 56s\n",
      "42:\tlearn: 0.5726132\ttotal: 10.6s\tremaining: 3m 55s\n",
      "44:\tlearn: 0.5537372\ttotal: 11s\tremaining: 3m 54s\n",
      "46:\tlearn: 0.5393484\ttotal: 11.4s\tremaining: 3m 51s\n",
      "48:\tlearn: 0.5208162\ttotal: 11.9s\tremaining: 3m 51s\n",
      "50:\tlearn: 0.5125713\ttotal: 12.3s\tremaining: 3m 49s\n",
      "52:\tlearn: 0.4922973\ttotal: 12.8s\tremaining: 3m 49s\n",
      "54:\tlearn: 0.4795289\ttotal: 13.3s\tremaining: 3m 48s\n",
      "56:\tlearn: 0.4695192\ttotal: 13.7s\tremaining: 3m 46s\n",
      "58:\tlearn: 0.4590443\ttotal: 14.1s\tremaining: 3m 44s\n",
      "60:\tlearn: 0.4494728\ttotal: 14.5s\tremaining: 3m 43s\n",
      "62:\tlearn: 0.4401861\ttotal: 15s\tremaining: 3m 42s\n",
      "64:\tlearn: 0.4304036\ttotal: 15.4s\tremaining: 3m 41s\n",
      "66:\tlearn: 0.4225964\ttotal: 15.8s\tremaining: 3m 39s\n",
      "68:\tlearn: 0.4113094\ttotal: 16.3s\tremaining: 3m 39s\n",
      "70:\tlearn: 0.4017322\ttotal: 16.7s\tremaining: 3m 38s\n",
      "72:\tlearn: 0.3911568\ttotal: 17.2s\tremaining: 3m 37s\n",
      "74:\tlearn: 0.3849836\ttotal: 17.6s\tremaining: 3m 36s\n",
      "76:\tlearn: 0.3802431\ttotal: 18s\tremaining: 3m 35s\n",
      "78:\tlearn: 0.3745376\ttotal: 18.5s\tremaining: 3m 35s\n",
      "80:\tlearn: 0.3688938\ttotal: 19.1s\tremaining: 3m 36s\n",
      "82:\tlearn: 0.3635634\ttotal: 19.5s\tremaining: 3m 35s\n",
      "84:\tlearn: 0.3574191\ttotal: 20s\tremaining: 3m 35s\n",
      "86:\tlearn: 0.3504235\ttotal: 20.5s\tremaining: 3m 35s\n",
      "88:\tlearn: 0.3429533\ttotal: 21s\tremaining: 3m 34s\n",
      "90:\tlearn: 0.3378561\ttotal: 21.4s\tremaining: 3m 33s\n",
      "92:\tlearn: 0.3342969\ttotal: 21.8s\tremaining: 3m 32s\n",
      "94:\tlearn: 0.3296151\ttotal: 22.2s\tremaining: 3m 31s\n",
      "96:\tlearn: 0.3238101\ttotal: 22.7s\tremaining: 3m 31s\n",
      "98:\tlearn: 0.3198928\ttotal: 23.1s\tremaining: 3m 30s\n",
      "100:\tlearn: 0.3151003\ttotal: 23.6s\tremaining: 3m 29s\n",
      "102:\tlearn: 0.3110144\ttotal: 24s\tremaining: 3m 29s\n",
      "104:\tlearn: 0.3052432\ttotal: 24.5s\tremaining: 3m 28s\n",
      "106:\tlearn: 0.3008529\ttotal: 24.9s\tremaining: 3m 27s\n",
      "108:\tlearn: 0.2957537\ttotal: 25.3s\tremaining: 3m 27s\n",
      "110:\tlearn: 0.2924530\ttotal: 25.7s\tremaining: 3m 25s\n",
      "112:\tlearn: 0.2896345\ttotal: 26.1s\tremaining: 3m 24s\n",
      "114:\tlearn: 0.2859364\ttotal: 26.5s\tremaining: 3m 24s\n",
      "116:\tlearn: 0.2834063\ttotal: 26.9s\tremaining: 3m 23s\n",
      "118:\tlearn: 0.2791145\ttotal: 27.3s\tremaining: 3m 22s\n",
      "120:\tlearn: 0.2777563\ttotal: 27.7s\tremaining: 3m 21s\n",
      "122:\tlearn: 0.2750182\ttotal: 28.2s\tremaining: 3m 20s\n",
      "124:\tlearn: 0.2720502\ttotal: 28.6s\tremaining: 3m 20s\n",
      "126:\tlearn: 0.2698345\ttotal: 29s\tremaining: 3m 19s\n",
      "128:\tlearn: 0.2670331\ttotal: 29.4s\tremaining: 3m 18s\n",
      "130:\tlearn: 0.2643696\ttotal: 29.8s\tremaining: 3m 17s\n",
      "132:\tlearn: 0.2613547\ttotal: 30.2s\tremaining: 3m 16s\n",
      "134:\tlearn: 0.2594214\ttotal: 30.6s\tremaining: 3m 16s\n",
      "136:\tlearn: 0.2575569\ttotal: 31s\tremaining: 3m 15s\n",
      "138:\tlearn: 0.2562454\ttotal: 31.4s\tremaining: 3m 14s\n",
      "140:\tlearn: 0.2540742\ttotal: 31.7s\tremaining: 3m 13s\n",
      "142:\tlearn: 0.2516578\ttotal: 32.1s\tremaining: 3m 12s\n",
      "144:\tlearn: 0.2500208\ttotal: 32.5s\tremaining: 3m 11s\n",
      "146:\tlearn: 0.2492489\ttotal: 32.9s\tremaining: 3m 10s\n",
      "148:\tlearn: 0.2478507\ttotal: 33.3s\tremaining: 3m 10s\n",
      "150:\tlearn: 0.2463826\ttotal: 33.7s\tremaining: 3m 9s\n",
      "152:\tlearn: 0.2441171\ttotal: 34.1s\tremaining: 3m 8s\n",
      "154:\tlearn: 0.2425247\ttotal: 34.5s\tremaining: 3m 7s\n",
      "156:\tlearn: 0.2416206\ttotal: 34.9s\tremaining: 3m 7s\n",
      "158:\tlearn: 0.2406669\ttotal: 35.3s\tremaining: 3m 6s\n",
      "160:\tlearn: 0.2394423\ttotal: 35.6s\tremaining: 3m 5s\n",
      "162:\tlearn: 0.2376949\ttotal: 36s\tremaining: 3m 4s\n",
      "164:\tlearn: 0.2350693\ttotal: 36.4s\tremaining: 3m 4s\n",
      "166:\tlearn: 0.2341459\ttotal: 36.8s\tremaining: 3m 3s\n",
      "168:\tlearn: 0.2328910\ttotal: 37.2s\tremaining: 3m 2s\n",
      "170:\tlearn: 0.2317415\ttotal: 37.6s\tremaining: 3m 2s\n",
      "172:\tlearn: 0.2303545\ttotal: 38s\tremaining: 3m 1s\n",
      "174:\tlearn: 0.2299202\ttotal: 38.4s\tremaining: 3m\n",
      "176:\tlearn: 0.2285399\ttotal: 38.8s\tremaining: 3m\n",
      "178:\tlearn: 0.2277272\ttotal: 39.2s\tremaining: 2m 59s\n",
      "180:\tlearn: 0.2264901\ttotal: 39.6s\tremaining: 2m 59s\n",
      "182:\tlearn: 0.2253992\ttotal: 40s\tremaining: 2m 58s\n",
      "184:\tlearn: 0.2244498\ttotal: 40.4s\tremaining: 2m 57s\n",
      "186:\tlearn: 0.2229610\ttotal: 40.8s\tremaining: 2m 57s\n",
      "188:\tlearn: 0.2220833\ttotal: 41.2s\tremaining: 2m 56s\n",
      "190:\tlearn: 0.2201050\ttotal: 41.6s\tremaining: 2m 56s\n",
      "192:\tlearn: 0.2198258\ttotal: 42s\tremaining: 2m 55s\n",
      "194:\tlearn: 0.2187073\ttotal: 42.4s\tremaining: 2m 55s\n",
      "196:\tlearn: 0.2175265\ttotal: 42.8s\tremaining: 2m 54s\n",
      "198:\tlearn: 0.2171791\ttotal: 43.1s\tremaining: 2m 53s\n",
      "200:\tlearn: 0.2153377\ttotal: 43.6s\tremaining: 2m 53s\n",
      "202:\tlearn: 0.2143578\ttotal: 43.9s\tremaining: 2m 52s\n",
      "204:\tlearn: 0.2140080\ttotal: 44.3s\tremaining: 2m 51s\n",
      "206:\tlearn: 0.2137960\ttotal: 44.6s\tremaining: 2m 50s\n",
      "208:\tlearn: 0.2124804\ttotal: 45s\tremaining: 2m 50s\n",
      "210:\tlearn: 0.2114095\ttotal: 45.4s\tremaining: 2m 49s\n",
      "212:\tlearn: 0.2101892\ttotal: 45.8s\tremaining: 2m 49s\n",
      "214:\tlearn: 0.2096682\ttotal: 46.1s\tremaining: 2m 48s\n",
      "216:\tlearn: 0.2081523\ttotal: 46.5s\tremaining: 2m 47s\n",
      "218:\tlearn: 0.2077989\ttotal: 46.9s\tremaining: 2m 47s\n",
      "220:\tlearn: 0.2071558\ttotal: 47.2s\tremaining: 2m 46s\n",
      "222:\tlearn: 0.2063437\ttotal: 47.6s\tremaining: 2m 45s\n",
      "224:\tlearn: 0.2060310\ttotal: 47.9s\tremaining: 2m 45s\n",
      "226:\tlearn: 0.2052030\ttotal: 48.3s\tremaining: 2m 44s\n",
      "228:\tlearn: 0.2042256\ttotal: 48.7s\tremaining: 2m 44s\n",
      "230:\tlearn: 0.2037950\ttotal: 49.1s\tremaining: 2m 43s\n",
      "232:\tlearn: 0.2037867\ttotal: 49.4s\tremaining: 2m 42s\n",
      "234:\tlearn: 0.2033549\ttotal: 49.8s\tremaining: 2m 42s\n",
      "236:\tlearn: 0.2029912\ttotal: 50.1s\tremaining: 2m 41s\n",
      "238:\tlearn: 0.2028545\ttotal: 50.4s\tremaining: 2m 40s\n",
      "240:\tlearn: 0.2025946\ttotal: 50.8s\tremaining: 2m 39s\n",
      "242:\tlearn: 0.2011575\ttotal: 51.2s\tremaining: 2m 39s\n",
      "244:\tlearn: 0.2001163\ttotal: 51.6s\tremaining: 2m 38s\n",
      "246:\tlearn: 0.2000369\ttotal: 51.9s\tremaining: 2m 38s\n",
      "248:\tlearn: 0.1996203\ttotal: 52.3s\tremaining: 2m 37s\n",
      "250:\tlearn: 0.1992456\ttotal: 52.6s\tremaining: 2m 36s\n",
      "252:\tlearn: 0.1985856\ttotal: 53s\tremaining: 2m 36s\n",
      "254:\tlearn: 0.1980897\ttotal: 53.3s\tremaining: 2m 35s\n",
      "256:\tlearn: 0.1978338\ttotal: 53.7s\tremaining: 2m 35s\n",
      "258:\tlearn: 0.1975598\ttotal: 54s\tremaining: 2m 34s\n",
      "260:\tlearn: 0.1967532\ttotal: 54.4s\tremaining: 2m 34s\n",
      "262:\tlearn: 0.1961383\ttotal: 54.9s\tremaining: 2m 33s\n",
      "264:\tlearn: 0.1956176\ttotal: 55.2s\tremaining: 2m 33s\n",
      "266:\tlearn: 0.1945266\ttotal: 55.6s\tremaining: 2m 32s\n",
      "268:\tlearn: 0.1937881\ttotal: 56s\tremaining: 2m 32s\n",
      "270:\tlearn: 0.1928669\ttotal: 56.4s\tremaining: 2m 31s\n",
      "272:\tlearn: 0.1916273\ttotal: 56.8s\tremaining: 2m 31s\n",
      "274:\tlearn: 0.1913534\ttotal: 57.1s\tremaining: 2m 30s\n",
      "276:\tlearn: 0.1899788\ttotal: 57.5s\tremaining: 2m 30s\n",
      "278:\tlearn: 0.1894378\ttotal: 57.9s\tremaining: 2m 29s\n",
      "280:\tlearn: 0.1891141\ttotal: 58.3s\tremaining: 2m 29s\n",
      "282:\tlearn: 0.1888467\ttotal: 58.6s\tremaining: 2m 28s\n",
      "284:\tlearn: 0.1883351\ttotal: 59s\tremaining: 2m 28s\n",
      "286:\tlearn: 0.1879953\ttotal: 59.4s\tremaining: 2m 27s\n",
      "288:\tlearn: 0.1878087\ttotal: 59.8s\tremaining: 2m 27s\n",
      "290:\tlearn: 0.1873069\ttotal: 1m\tremaining: 2m 26s\n",
      "292:\tlearn: 0.1871016\ttotal: 1m\tremaining: 2m 25s\n",
      "294:\tlearn: 0.1861055\ttotal: 1m\tremaining: 2m 25s\n",
      "296:\tlearn: 0.1854323\ttotal: 1m 1s\tremaining: 2m 25s\n",
      "298:\tlearn: 0.1847052\ttotal: 1m 1s\tremaining: 2m 24s\n",
      "300:\tlearn: 0.1844950\ttotal: 1m 2s\tremaining: 2m 24s\n",
      "302:\tlearn: 0.1841090\ttotal: 1m 2s\tremaining: 2m 23s\n",
      "304:\tlearn: 0.1838497\ttotal: 1m 2s\tremaining: 2m 23s\n",
      "306:\tlearn: 0.1826924\ttotal: 1m 3s\tremaining: 2m 22s\n",
      "308:\tlearn: 0.1823176\ttotal: 1m 3s\tremaining: 2m 22s\n",
      "310:\tlearn: 0.1818911\ttotal: 1m 3s\tremaining: 2m 21s\n",
      "312:\tlearn: 0.1818827\ttotal: 1m 4s\tremaining: 2m 21s\n",
      "314:\tlearn: 0.1817029\ttotal: 1m 4s\tremaining: 2m 20s\n",
      "316:\tlearn: 0.1813077\ttotal: 1m 4s\tremaining: 2m 19s\n",
      "318:\tlearn: 0.1811772\ttotal: 1m 5s\tremaining: 2m 19s\n",
      "320:\tlearn: 0.1811015\ttotal: 1m 5s\tremaining: 2m 18s\n",
      "322:\tlearn: 0.1809082\ttotal: 1m 6s\tremaining: 2m 18s\n",
      "324:\tlearn: 0.1803608\ttotal: 1m 6s\tremaining: 2m 18s\n",
      "326:\tlearn: 0.1800011\ttotal: 1m 6s\tremaining: 2m 17s\n",
      "328:\tlearn: 0.1797729\ttotal: 1m 7s\tremaining: 2m 17s\n",
      "330:\tlearn: 0.1794240\ttotal: 1m 7s\tremaining: 2m 16s\n",
      "332:\tlearn: 0.1786908\ttotal: 1m 8s\tremaining: 2m 16s\n",
      "334:\tlearn: 0.1785320\ttotal: 1m 8s\tremaining: 2m 15s\n",
      "336:\tlearn: 0.1783326\ttotal: 1m 8s\tremaining: 2m 15s\n",
      "338:\tlearn: 0.1781994\ttotal: 1m 9s\tremaining: 2m 14s\n",
      "340:\tlearn: 0.1773484\ttotal: 1m 9s\tremaining: 2m 14s\n",
      "342:\tlearn: 0.1768400\ttotal: 1m 9s\tremaining: 2m 14s\n",
      "344:\tlearn: 0.1766517\ttotal: 1m 10s\tremaining: 2m 13s\n",
      "346:\tlearn: 0.1766439\ttotal: 1m 10s\tremaining: 2m 13s\n",
      "348:\tlearn: 0.1764560\ttotal: 1m 11s\tremaining: 2m 12s\n",
      "350:\tlearn: 0.1761893\ttotal: 1m 11s\tremaining: 2m 12s\n",
      "352:\tlearn: 0.1761048\ttotal: 1m 11s\tremaining: 2m 11s\n",
      "354:\tlearn: 0.1757659\ttotal: 1m 12s\tremaining: 2m 11s\n",
      "356:\tlearn: 0.1755941\ttotal: 1m 12s\tremaining: 2m 10s\n",
      "358:\tlearn: 0.1754139\ttotal: 1m 12s\tremaining: 2m 10s\n",
      "360:\tlearn: 0.1749810\ttotal: 1m 13s\tremaining: 2m 9s\n",
      "362:\tlearn: 0.1745601\ttotal: 1m 13s\tremaining: 2m 9s\n",
      "364:\tlearn: 0.1738160\ttotal: 1m 13s\tremaining: 2m 8s\n",
      "366:\tlearn: 0.1736736\ttotal: 1m 14s\tremaining: 2m 8s\n",
      "368:\tlearn: 0.1735347\ttotal: 1m 14s\tremaining: 2m 7s\n",
      "370:\tlearn: 0.1725459\ttotal: 1m 15s\tremaining: 2m 7s\n",
      "372:\tlearn: 0.1718324\ttotal: 1m 15s\tremaining: 2m 6s\n",
      "374:\tlearn: 0.1715172\ttotal: 1m 15s\tremaining: 2m 6s\n",
      "376:\tlearn: 0.1713348\ttotal: 1m 16s\tremaining: 2m 5s\n",
      "378:\tlearn: 0.1712140\ttotal: 1m 16s\tremaining: 2m 5s\n",
      "380:\tlearn: 0.1709176\ttotal: 1m 16s\tremaining: 2m 4s\n",
      "382:\tlearn: 0.1708392\ttotal: 1m 17s\tremaining: 2m 4s\n",
      "384:\tlearn: 0.1705380\ttotal: 1m 17s\tremaining: 2m 4s\n",
      "386:\tlearn: 0.1703409\ttotal: 1m 18s\tremaining: 2m 3s\n",
      "388:\tlearn: 0.1702618\ttotal: 1m 18s\tremaining: 2m 3s\n",
      "390:\tlearn: 0.1698322\ttotal: 1m 18s\tremaining: 2m 2s\n",
      "392:\tlearn: 0.1694415\ttotal: 1m 19s\tremaining: 2m 2s\n",
      "394:\tlearn: 0.1691045\ttotal: 1m 19s\tremaining: 2m 1s\n",
      "396:\tlearn: 0.1687976\ttotal: 1m 19s\tremaining: 2m 1s\n",
      "398:\tlearn: 0.1682418\ttotal: 1m 20s\tremaining: 2m\n",
      "400:\tlearn: 0.1678827\ttotal: 1m 20s\tremaining: 2m\n",
      "402:\tlearn: 0.1678447\ttotal: 1m 20s\tremaining: 1m 59s\n",
      "404:\tlearn: 0.1677309\ttotal: 1m 21s\tremaining: 1m 59s\n",
      "406:\tlearn: 0.1676769\ttotal: 1m 21s\tremaining: 1m 58s\n",
      "408:\tlearn: 0.1673390\ttotal: 1m 22s\tremaining: 1m 58s\n",
      "410:\tlearn: 0.1672799\ttotal: 1m 22s\tremaining: 1m 58s\n",
      "412:\tlearn: 0.1672019\ttotal: 1m 22s\tremaining: 1m 57s\n",
      "414:\tlearn: 0.1670584\ttotal: 1m 23s\tremaining: 1m 57s\n",
      "416:\tlearn: 0.1663564\ttotal: 1m 23s\tremaining: 1m 56s\n",
      "418:\tlearn: 0.1659538\ttotal: 1m 23s\tremaining: 1m 56s\n",
      "420:\tlearn: 0.1653978\ttotal: 1m 24s\tremaining: 1m 55s\n",
      "422:\tlearn: 0.1653233\ttotal: 1m 24s\tremaining: 1m 55s\n",
      "424:\tlearn: 0.1652241\ttotal: 1m 24s\tremaining: 1m 54s\n",
      "426:\tlearn: 0.1649861\ttotal: 1m 25s\tremaining: 1m 54s\n",
      "428:\tlearn: 0.1646214\ttotal: 1m 25s\tremaining: 1m 54s\n",
      "430:\tlearn: 0.1643653\ttotal: 1m 26s\tremaining: 1m 53s\n",
      "432:\tlearn: 0.1642423\ttotal: 1m 26s\tremaining: 1m 53s\n",
      "434:\tlearn: 0.1641531\ttotal: 1m 26s\tremaining: 1m 52s\n",
      "436:\tlearn: 0.1639995\ttotal: 1m 27s\tremaining: 1m 52s\n",
      "438:\tlearn: 0.1639310\ttotal: 1m 27s\tremaining: 1m 51s\n",
      "440:\tlearn: 0.1636933\ttotal: 1m 27s\tremaining: 1m 51s\n",
      "442:\tlearn: 0.1634371\ttotal: 1m 28s\tremaining: 1m 50s\n",
      "444:\tlearn: 0.1633882\ttotal: 1m 28s\tremaining: 1m 50s\n",
      "446:\tlearn: 0.1632654\ttotal: 1m 28s\tremaining: 1m 49s\n",
      "448:\tlearn: 0.1629791\ttotal: 1m 29s\tremaining: 1m 49s\n",
      "450:\tlearn: 0.1627404\ttotal: 1m 29s\tremaining: 1m 49s\n",
      "452:\tlearn: 0.1625388\ttotal: 1m 30s\tremaining: 1m 48s\n",
      "454:\tlearn: 0.1624629\ttotal: 1m 30s\tremaining: 1m 48s\n",
      "456:\tlearn: 0.1623071\ttotal: 1m 30s\tremaining: 1m 47s\n",
      "458:\tlearn: 0.1621632\ttotal: 1m 31s\tremaining: 1m 47s\n",
      "460:\tlearn: 0.1617623\ttotal: 1m 31s\tremaining: 1m 46s\n",
      "462:\tlearn: 0.1613377\ttotal: 1m 31s\tremaining: 1m 46s\n",
      "464:\tlearn: 0.1610776\ttotal: 1m 32s\tremaining: 1m 46s\n",
      "466:\tlearn: 0.1604579\ttotal: 1m 32s\tremaining: 1m 45s\n",
      "468:\tlearn: 0.1603529\ttotal: 1m 32s\tremaining: 1m 45s\n",
      "470:\tlearn: 0.1602296\ttotal: 1m 33s\tremaining: 1m 44s\n",
      "472:\tlearn: 0.1599173\ttotal: 1m 33s\tremaining: 1m 44s\n",
      "474:\tlearn: 0.1597024\ttotal: 1m 34s\tremaining: 1m 43s\n",
      "476:\tlearn: 0.1596482\ttotal: 1m 34s\tremaining: 1m 43s\n",
      "478:\tlearn: 0.1592103\ttotal: 1m 34s\tremaining: 1m 43s\n",
      "480:\tlearn: 0.1592046\ttotal: 1m 35s\tremaining: 1m 42s\n",
      "482:\tlearn: 0.1583960\ttotal: 1m 35s\tremaining: 1m 42s\n",
      "484:\tlearn: 0.1577217\ttotal: 1m 35s\tremaining: 1m 41s\n",
      "486:\tlearn: 0.1576013\ttotal: 1m 36s\tremaining: 1m 41s\n",
      "488:\tlearn: 0.1575309\ttotal: 1m 36s\tremaining: 1m 40s\n",
      "490:\tlearn: 0.1569946\ttotal: 1m 36s\tremaining: 1m 40s\n",
      "492:\tlearn: 0.1568267\ttotal: 1m 37s\tremaining: 1m 40s\n",
      "494:\tlearn: 0.1566513\ttotal: 1m 37s\tremaining: 1m 39s\n",
      "496:\tlearn: 0.1556181\ttotal: 1m 38s\tremaining: 1m 39s\n",
      "498:\tlearn: 0.1553693\ttotal: 1m 38s\tremaining: 1m 38s\n",
      "500:\tlearn: 0.1549261\ttotal: 1m 38s\tremaining: 1m 38s\n",
      "502:\tlearn: 0.1548211\ttotal: 1m 39s\tremaining: 1m 38s\n",
      "504:\tlearn: 0.1547612\ttotal: 1m 39s\tremaining: 1m 37s\n",
      "506:\tlearn: 0.1545116\ttotal: 1m 40s\tremaining: 1m 37s\n",
      "508:\tlearn: 0.1543683\ttotal: 1m 40s\tremaining: 1m 36s\n",
      "510:\tlearn: 0.1542277\ttotal: 1m 40s\tremaining: 1m 36s\n",
      "512:\tlearn: 0.1541580\ttotal: 1m 41s\tremaining: 1m 35s\n",
      "514:\tlearn: 0.1540484\ttotal: 1m 41s\tremaining: 1m 35s\n",
      "516:\tlearn: 0.1539493\ttotal: 1m 41s\tremaining: 1m 35s\n",
      "518:\tlearn: 0.1535494\ttotal: 1m 42s\tremaining: 1m 34s\n",
      "520:\tlearn: 0.1530448\ttotal: 1m 42s\tremaining: 1m 34s\n",
      "522:\tlearn: 0.1529424\ttotal: 1m 43s\tremaining: 1m 33s\n",
      "524:\tlearn: 0.1527800\ttotal: 1m 43s\tremaining: 1m 33s\n",
      "526:\tlearn: 0.1527238\ttotal: 1m 43s\tremaining: 1m 33s\n",
      "528:\tlearn: 0.1519449\ttotal: 1m 44s\tremaining: 1m 32s\n",
      "530:\tlearn: 0.1517824\ttotal: 1m 44s\tremaining: 1m 32s\n",
      "532:\tlearn: 0.1515124\ttotal: 1m 44s\tremaining: 1m 31s\n",
      "534:\tlearn: 0.1512021\ttotal: 1m 45s\tremaining: 1m 31s\n",
      "536:\tlearn: 0.1509946\ttotal: 1m 45s\tremaining: 1m 31s\n",
      "538:\tlearn: 0.1506063\ttotal: 1m 45s\tremaining: 1m 30s\n",
      "540:\tlearn: 0.1505596\ttotal: 1m 46s\tremaining: 1m 30s\n",
      "542:\tlearn: 0.1502999\ttotal: 1m 46s\tremaining: 1m 29s\n",
      "544:\tlearn: 0.1499276\ttotal: 1m 47s\tremaining: 1m 29s\n",
      "546:\tlearn: 0.1498592\ttotal: 1m 47s\tremaining: 1m 28s\n",
      "548:\tlearn: 0.1494967\ttotal: 1m 47s\tremaining: 1m 28s\n",
      "550:\tlearn: 0.1493893\ttotal: 1m 48s\tremaining: 1m 28s\n",
      "552:\tlearn: 0.1492540\ttotal: 1m 48s\tremaining: 1m 27s\n",
      "554:\tlearn: 0.1490293\ttotal: 1m 48s\tremaining: 1m 27s\n",
      "556:\tlearn: 0.1489561\ttotal: 1m 49s\tremaining: 1m 26s\n",
      "558:\tlearn: 0.1488558\ttotal: 1m 49s\tremaining: 1m 26s\n",
      "560:\tlearn: 0.1485968\ttotal: 1m 49s\tremaining: 1m 26s\n",
      "562:\tlearn: 0.1484032\ttotal: 1m 50s\tremaining: 1m 25s\n",
      "564:\tlearn: 0.1482377\ttotal: 1m 50s\tremaining: 1m 25s\n",
      "566:\tlearn: 0.1481587\ttotal: 1m 51s\tremaining: 1m 24s\n",
      "568:\tlearn: 0.1480882\ttotal: 1m 51s\tremaining: 1m 24s\n",
      "570:\tlearn: 0.1479779\ttotal: 1m 51s\tremaining: 1m 23s\n",
      "572:\tlearn: 0.1478851\ttotal: 1m 52s\tremaining: 1m 23s\n",
      "574:\tlearn: 0.1477428\ttotal: 1m 52s\tremaining: 1m 23s\n",
      "576:\tlearn: 0.1476337\ttotal: 1m 52s\tremaining: 1m 22s\n",
      "578:\tlearn: 0.1475013\ttotal: 1m 53s\tremaining: 1m 22s\n",
      "580:\tlearn: 0.1473873\ttotal: 1m 53s\tremaining: 1m 21s\n",
      "582:\tlearn: 0.1471303\ttotal: 1m 53s\tremaining: 1m 21s\n",
      "584:\tlearn: 0.1469702\ttotal: 1m 54s\tremaining: 1m 21s\n",
      "586:\tlearn: 0.1466759\ttotal: 1m 54s\tremaining: 1m 20s\n",
      "588:\tlearn: 0.1464904\ttotal: 1m 54s\tremaining: 1m 20s\n",
      "590:\tlearn: 0.1462161\ttotal: 1m 55s\tremaining: 1m 19s\n",
      "592:\tlearn: 0.1460324\ttotal: 1m 55s\tremaining: 1m 19s\n",
      "594:\tlearn: 0.1459259\ttotal: 1m 56s\tremaining: 1m 18s\n",
      "596:\tlearn: 0.1458561\ttotal: 1m 56s\tremaining: 1m 18s\n",
      "598:\tlearn: 0.1457843\ttotal: 1m 56s\tremaining: 1m 18s\n",
      "600:\tlearn: 0.1455065\ttotal: 1m 57s\tremaining: 1m 17s\n",
      "602:\tlearn: 0.1453567\ttotal: 1m 57s\tremaining: 1m 17s\n",
      "604:\tlearn: 0.1452916\ttotal: 1m 57s\tremaining: 1m 16s\n",
      "606:\tlearn: 0.1450305\ttotal: 1m 58s\tremaining: 1m 16s\n",
      "608:\tlearn: 0.1449707\ttotal: 1m 58s\tremaining: 1m 16s\n",
      "610:\tlearn: 0.1449649\ttotal: 1m 58s\tremaining: 1m 15s\n",
      "612:\tlearn: 0.1449549\ttotal: 1m 59s\tremaining: 1m 15s\n",
      "614:\tlearn: 0.1444614\ttotal: 1m 59s\tremaining: 1m 14s\n",
      "616:\tlearn: 0.1442969\ttotal: 1m 59s\tremaining: 1m 14s\n",
      "618:\tlearn: 0.1440965\ttotal: 2m\tremaining: 1m 14s\n",
      "620:\tlearn: 0.1439163\ttotal: 2m\tremaining: 1m 13s\n",
      "622:\tlearn: 0.1437460\ttotal: 2m 1s\tremaining: 1m 13s\n",
      "624:\tlearn: 0.1435459\ttotal: 2m 1s\tremaining: 1m 12s\n",
      "626:\tlearn: 0.1432202\ttotal: 2m 1s\tremaining: 1m 12s\n",
      "628:\tlearn: 0.1430339\ttotal: 2m 2s\tremaining: 1m 12s\n",
      "630:\tlearn: 0.1428063\ttotal: 2m 2s\tremaining: 1m 11s\n",
      "632:\tlearn: 0.1426756\ttotal: 2m 2s\tremaining: 1m 11s\n",
      "634:\tlearn: 0.1426230\ttotal: 2m 3s\tremaining: 1m 10s\n",
      "636:\tlearn: 0.1425267\ttotal: 2m 3s\tremaining: 1m 10s\n",
      "638:\tlearn: 0.1424477\ttotal: 2m 3s\tremaining: 1m 10s\n",
      "640:\tlearn: 0.1422299\ttotal: 2m 4s\tremaining: 1m 9s\n",
      "642:\tlearn: 0.1421416\ttotal: 2m 4s\tremaining: 1m 9s\n",
      "644:\tlearn: 0.1419814\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "646:\tlearn: 0.1418534\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "648:\tlearn: 0.1415520\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "650:\tlearn: 0.1414706\ttotal: 2m 6s\tremaining: 1m 7s\n",
      "652:\tlearn: 0.1414020\ttotal: 2m 6s\tremaining: 1m 7s\n",
      "654:\tlearn: 0.1408873\ttotal: 2m 7s\tremaining: 1m 6s\n",
      "656:\tlearn: 0.1407965\ttotal: 2m 7s\tremaining: 1m 6s\n",
      "658:\tlearn: 0.1407494\ttotal: 2m 7s\tremaining: 1m 6s\n",
      "660:\tlearn: 0.1406107\ttotal: 2m 8s\tremaining: 1m 5s\n",
      "662:\tlearn: 0.1405583\ttotal: 2m 8s\tremaining: 1m 5s\n",
      "664:\tlearn: 0.1405032\ttotal: 2m 8s\tremaining: 1m 4s\n",
      "666:\tlearn: 0.1402278\ttotal: 2m 9s\tremaining: 1m 4s\n",
      "668:\tlearn: 0.1400684\ttotal: 2m 9s\tremaining: 1m 4s\n",
      "670:\tlearn: 0.1399773\ttotal: 2m 10s\tremaining: 1m 3s\n",
      "672:\tlearn: 0.1398835\ttotal: 2m 10s\tremaining: 1m 3s\n",
      "674:\tlearn: 0.1397984\ttotal: 2m 10s\tremaining: 1m 2s\n",
      "676:\tlearn: 0.1396783\ttotal: 2m 11s\tremaining: 1m 2s\n",
      "678:\tlearn: 0.1394995\ttotal: 2m 11s\tremaining: 1m 2s\n",
      "680:\tlearn: 0.1393178\ttotal: 2m 11s\tremaining: 1m 1s\n",
      "682:\tlearn: 0.1392259\ttotal: 2m 12s\tremaining: 1m 1s\n",
      "684:\tlearn: 0.1390315\ttotal: 2m 12s\tremaining: 1m\n",
      "686:\tlearn: 0.1386256\ttotal: 2m 12s\tremaining: 1m\n",
      "688:\tlearn: 0.1384845\ttotal: 2m 13s\tremaining: 1m\n",
      "690:\tlearn: 0.1384103\ttotal: 2m 13s\tremaining: 59.8s\n",
      "692:\tlearn: 0.1383596\ttotal: 2m 14s\tremaining: 59.4s\n",
      "694:\tlearn: 0.1382498\ttotal: 2m 14s\tremaining: 59s\n",
      "696:\tlearn: 0.1381839\ttotal: 2m 14s\tremaining: 58.6s\n",
      "698:\tlearn: 0.1378452\ttotal: 2m 15s\tremaining: 58.2s\n",
      "700:\tlearn: 0.1376415\ttotal: 2m 15s\tremaining: 57.8s\n",
      "702:\tlearn: 0.1375951\ttotal: 2m 15s\tremaining: 57.4s\n",
      "704:\tlearn: 0.1373495\ttotal: 2m 16s\tremaining: 57s\n",
      "706:\tlearn: 0.1371441\ttotal: 2m 16s\tremaining: 56.6s\n",
      "708:\tlearn: 0.1370113\ttotal: 2m 17s\tremaining: 56.2s\n",
      "710:\tlearn: 0.1369760\ttotal: 2m 17s\tremaining: 55.8s\n",
      "712:\tlearn: 0.1368132\ttotal: 2m 17s\tremaining: 55.4s\n",
      "714:\tlearn: 0.1367404\ttotal: 2m 18s\tremaining: 55s\n",
      "716:\tlearn: 0.1366710\ttotal: 2m 18s\tremaining: 54.6s\n",
      "718:\tlearn: 0.1365279\ttotal: 2m 18s\tremaining: 54.2s\n",
      "720:\tlearn: 0.1363978\ttotal: 2m 19s\tremaining: 53.8s\n",
      "722:\tlearn: 0.1363461\ttotal: 2m 19s\tremaining: 53.4s\n",
      "724:\tlearn: 0.1362878\ttotal: 2m 19s\tremaining: 53s\n",
      "726:\tlearn: 0.1362341\ttotal: 2m 20s\tremaining: 52.6s\n",
      "728:\tlearn: 0.1362310\ttotal: 2m 20s\tremaining: 52.2s\n",
      "730:\tlearn: 0.1361497\ttotal: 2m 20s\tremaining: 51.9s\n",
      "732:\tlearn: 0.1360300\ttotal: 2m 21s\tremaining: 51.5s\n",
      "734:\tlearn: 0.1359724\ttotal: 2m 21s\tremaining: 51.1s\n",
      "736:\tlearn: 0.1358804\ttotal: 2m 21s\tremaining: 50.7s\n",
      "738:\tlearn: 0.1358749\ttotal: 2m 22s\tremaining: 50.3s\n",
      "740:\tlearn: 0.1357226\ttotal: 2m 22s\tremaining: 49.9s\n",
      "742:\tlearn: 0.1354905\ttotal: 2m 23s\tremaining: 49.5s\n",
      "744:\tlearn: 0.1354373\ttotal: 2m 23s\tremaining: 49.1s\n",
      "746:\tlearn: 0.1353721\ttotal: 2m 23s\tremaining: 48.7s\n",
      "748:\tlearn: 0.1351620\ttotal: 2m 24s\tremaining: 48.3s\n",
      "750:\tlearn: 0.1351102\ttotal: 2m 24s\tremaining: 47.9s\n",
      "752:\tlearn: 0.1351052\ttotal: 2m 24s\tremaining: 47.5s\n",
      "754:\tlearn: 0.1350487\ttotal: 2m 25s\tremaining: 47.1s\n",
      "756:\tlearn: 0.1348921\ttotal: 2m 25s\tremaining: 46.7s\n",
      "758:\tlearn: 0.1347975\ttotal: 2m 25s\tremaining: 46.3s\n",
      "760:\tlearn: 0.1347587\ttotal: 2m 26s\tremaining: 45.9s\n",
      "762:\tlearn: 0.1346857\ttotal: 2m 26s\tremaining: 45.5s\n",
      "764:\tlearn: 0.1345512\ttotal: 2m 26s\tremaining: 45.1s\n",
      "766:\tlearn: 0.1344811\ttotal: 2m 27s\tremaining: 44.7s\n",
      "768:\tlearn: 0.1343678\ttotal: 2m 27s\tremaining: 44.3s\n",
      "770:\tlearn: 0.1343224\ttotal: 2m 27s\tremaining: 44s\n",
      "772:\tlearn: 0.1342171\ttotal: 2m 28s\tremaining: 43.6s\n",
      "774:\tlearn: 0.1341452\ttotal: 2m 28s\tremaining: 43.2s\n",
      "776:\tlearn: 0.1340725\ttotal: 2m 29s\tremaining: 42.8s\n",
      "778:\tlearn: 0.1339139\ttotal: 2m 29s\tremaining: 42.4s\n",
      "780:\tlearn: 0.1338758\ttotal: 2m 29s\tremaining: 42s\n",
      "782:\tlearn: 0.1338464\ttotal: 2m 30s\tremaining: 41.6s\n",
      "784:\tlearn: 0.1337827\ttotal: 2m 30s\tremaining: 41.3s\n",
      "786:\tlearn: 0.1337176\ttotal: 2m 30s\tremaining: 40.9s\n",
      "788:\tlearn: 0.1335301\ttotal: 2m 31s\tremaining: 40.5s\n",
      "790:\tlearn: 0.1333852\ttotal: 2m 31s\tremaining: 40.1s\n",
      "792:\tlearn: 0.1333157\ttotal: 2m 32s\tremaining: 39.7s\n",
      "794:\tlearn: 0.1331061\ttotal: 2m 32s\tremaining: 39.3s\n",
      "796:\tlearn: 0.1328831\ttotal: 2m 32s\tremaining: 38.9s\n",
      "798:\tlearn: 0.1328464\ttotal: 2m 33s\tremaining: 38.5s\n",
      "800:\tlearn: 0.1328152\ttotal: 2m 33s\tremaining: 38.2s\n",
      "802:\tlearn: 0.1327928\ttotal: 2m 33s\tremaining: 37.8s\n",
      "804:\tlearn: 0.1325994\ttotal: 2m 34s\tremaining: 37.4s\n",
      "806:\tlearn: 0.1323771\ttotal: 2m 34s\tremaining: 37s\n",
      "808:\tlearn: 0.1321883\ttotal: 2m 35s\tremaining: 36.6s\n",
      "810:\tlearn: 0.1320257\ttotal: 2m 35s\tremaining: 36.2s\n",
      "812:\tlearn: 0.1319785\ttotal: 2m 35s\tremaining: 35.8s\n",
      "814:\tlearn: 0.1319213\ttotal: 2m 36s\tremaining: 35.4s\n",
      "816:\tlearn: 0.1318728\ttotal: 2m 36s\tremaining: 35s\n",
      "818:\tlearn: 0.1316674\ttotal: 2m 36s\tremaining: 34.7s\n",
      "820:\tlearn: 0.1315911\ttotal: 2m 37s\tremaining: 34.3s\n",
      "822:\tlearn: 0.1314382\ttotal: 2m 37s\tremaining: 33.9s\n",
      "824:\tlearn: 0.1313825\ttotal: 2m 37s\tremaining: 33.5s\n",
      "826:\tlearn: 0.1313394\ttotal: 2m 38s\tremaining: 33.1s\n",
      "828:\tlearn: 0.1311559\ttotal: 2m 38s\tremaining: 32.7s\n",
      "830:\tlearn: 0.1309205\ttotal: 2m 39s\tremaining: 32.3s\n",
      "832:\tlearn: 0.1308370\ttotal: 2m 39s\tremaining: 32s\n",
      "834:\tlearn: 0.1307744\ttotal: 2m 39s\tremaining: 31.6s\n",
      "836:\tlearn: 0.1307329\ttotal: 2m 40s\tremaining: 31.2s\n",
      "838:\tlearn: 0.1304866\ttotal: 2m 40s\tremaining: 30.8s\n",
      "840:\tlearn: 0.1304134\ttotal: 2m 40s\tremaining: 30.4s\n",
      "842:\tlearn: 0.1302322\ttotal: 2m 41s\tremaining: 30s\n",
      "844:\tlearn: 0.1301314\ttotal: 2m 41s\tremaining: 29.6s\n",
      "846:\tlearn: 0.1300925\ttotal: 2m 41s\tremaining: 29.3s\n",
      "848:\tlearn: 0.1300155\ttotal: 2m 42s\tremaining: 28.9s\n",
      "850:\tlearn: 0.1299525\ttotal: 2m 42s\tremaining: 28.5s\n",
      "852:\tlearn: 0.1298611\ttotal: 2m 43s\tremaining: 28.1s\n",
      "854:\tlearn: 0.1296619\ttotal: 2m 43s\tremaining: 27.7s\n",
      "856:\tlearn: 0.1294818\ttotal: 2m 43s\tremaining: 27.3s\n",
      "858:\tlearn: 0.1294309\ttotal: 2m 44s\tremaining: 26.9s\n",
      "860:\tlearn: 0.1294091\ttotal: 2m 44s\tremaining: 26.6s\n",
      "862:\tlearn: 0.1293254\ttotal: 2m 44s\tremaining: 26.2s\n",
      "864:\tlearn: 0.1292284\ttotal: 2m 45s\tremaining: 25.8s\n",
      "866:\tlearn: 0.1290561\ttotal: 2m 45s\tremaining: 25.4s\n",
      "868:\tlearn: 0.1289244\ttotal: 2m 45s\tremaining: 25s\n",
      "870:\tlearn: 0.1288019\ttotal: 2m 46s\tremaining: 24.6s\n",
      "872:\tlearn: 0.1287229\ttotal: 2m 46s\tremaining: 24.2s\n",
      "874:\tlearn: 0.1286365\ttotal: 2m 46s\tremaining: 23.9s\n",
      "876:\tlearn: 0.1278057\ttotal: 2m 47s\tremaining: 23.5s\n",
      "878:\tlearn: 0.1277544\ttotal: 2m 47s\tremaining: 23.1s\n",
      "880:\tlearn: 0.1277052\ttotal: 2m 48s\tremaining: 22.7s\n",
      "882:\tlearn: 0.1276694\ttotal: 2m 48s\tremaining: 22.3s\n",
      "884:\tlearn: 0.1275663\ttotal: 2m 48s\tremaining: 21.9s\n",
      "886:\tlearn: 0.1275362\ttotal: 2m 49s\tremaining: 21.5s\n",
      "888:\tlearn: 0.1272641\ttotal: 2m 49s\tremaining: 21.2s\n",
      "890:\tlearn: 0.1270946\ttotal: 2m 49s\tremaining: 20.8s\n",
      "892:\tlearn: 0.1269271\ttotal: 2m 50s\tremaining: 20.4s\n",
      "894:\tlearn: 0.1268096\ttotal: 2m 50s\tremaining: 20s\n",
      "896:\tlearn: 0.1267469\ttotal: 2m 50s\tremaining: 19.6s\n",
      "898:\tlearn: 0.1266657\ttotal: 2m 51s\tremaining: 19.2s\n",
      "900:\tlearn: 0.1266195\ttotal: 2m 51s\tremaining: 18.9s\n",
      "902:\tlearn: 0.1265958\ttotal: 2m 52s\tremaining: 18.5s\n",
      "904:\tlearn: 0.1265082\ttotal: 2m 52s\tremaining: 18.1s\n",
      "906:\tlearn: 0.1264475\ttotal: 2m 52s\tremaining: 17.7s\n",
      "908:\tlearn: 0.1262518\ttotal: 2m 53s\tremaining: 17.3s\n",
      "910:\tlearn: 0.1260741\ttotal: 2m 53s\tremaining: 16.9s\n",
      "912:\tlearn: 0.1258540\ttotal: 2m 53s\tremaining: 16.6s\n",
      "914:\tlearn: 0.1256108\ttotal: 2m 54s\tremaining: 16.2s\n",
      "916:\tlearn: 0.1255400\ttotal: 2m 54s\tremaining: 15.8s\n",
      "918:\tlearn: 0.1254676\ttotal: 2m 54s\tremaining: 15.4s\n",
      "920:\tlearn: 0.1253267\ttotal: 2m 55s\tremaining: 15s\n",
      "922:\tlearn: 0.1252301\ttotal: 2m 55s\tremaining: 14.7s\n",
      "924:\tlearn: 0.1249214\ttotal: 2m 56s\tremaining: 14.3s\n",
      "926:\tlearn: 0.1248841\ttotal: 2m 56s\tremaining: 13.9s\n",
      "928:\tlearn: 0.1247472\ttotal: 2m 56s\tremaining: 13.5s\n",
      "930:\tlearn: 0.1246782\ttotal: 2m 57s\tremaining: 13.1s\n",
      "932:\tlearn: 0.1245075\ttotal: 2m 57s\tremaining: 12.8s\n",
      "934:\tlearn: 0.1244083\ttotal: 2m 57s\tremaining: 12.4s\n",
      "936:\tlearn: 0.1239913\ttotal: 2m 58s\tremaining: 12s\n",
      "938:\tlearn: 0.1239527\ttotal: 2m 58s\tremaining: 11.6s\n",
      "940:\tlearn: 0.1238419\ttotal: 2m 59s\tremaining: 11.2s\n",
      "942:\tlearn: 0.1237539\ttotal: 2m 59s\tremaining: 10.8s\n",
      "944:\tlearn: 0.1235467\ttotal: 2m 59s\tremaining: 10.5s\n",
      "946:\tlearn: 0.1235206\ttotal: 3m\tremaining: 10.1s\n",
      "948:\tlearn: 0.1234301\ttotal: 3m\tremaining: 9.7s\n",
      "950:\tlearn: 0.1234263\ttotal: 3m\tremaining: 9.32s\n",
      "952:\tlearn: 0.1233305\ttotal: 3m 1s\tremaining: 8.94s\n",
      "954:\tlearn: 0.1232504\ttotal: 3m 1s\tremaining: 8.56s\n",
      "956:\tlearn: 0.1231168\ttotal: 3m 1s\tremaining: 8.18s\n",
      "958:\tlearn: 0.1229858\ttotal: 3m 2s\tremaining: 7.79s\n",
      "960:\tlearn: 0.1229677\ttotal: 3m 2s\tremaining: 7.41s\n",
      "962:\tlearn: 0.1227834\ttotal: 3m 3s\tremaining: 7.03s\n",
      "964:\tlearn: 0.1227643\ttotal: 3m 3s\tremaining: 6.65s\n",
      "966:\tlearn: 0.1226754\ttotal: 3m 3s\tremaining: 6.27s\n",
      "968:\tlearn: 0.1225968\ttotal: 3m 4s\tremaining: 5.89s\n",
      "970:\tlearn: 0.1225362\ttotal: 3m 4s\tremaining: 5.51s\n",
      "972:\tlearn: 0.1224048\ttotal: 3m 4s\tremaining: 5.13s\n",
      "974:\tlearn: 0.1223004\ttotal: 3m 5s\tremaining: 4.75s\n",
      "976:\tlearn: 0.1221501\ttotal: 3m 5s\tremaining: 4.37s\n",
      "978:\tlearn: 0.1215848\ttotal: 3m 5s\tremaining: 3.99s\n",
      "980:\tlearn: 0.1214886\ttotal: 3m 6s\tremaining: 3.61s\n",
      "982:\tlearn: 0.1210478\ttotal: 3m 6s\tremaining: 3.23s\n",
      "984:\tlearn: 0.1209924\ttotal: 3m 7s\tremaining: 2.85s\n",
      "986:\tlearn: 0.1209164\ttotal: 3m 7s\tremaining: 2.47s\n",
      "988:\tlearn: 0.1208639\ttotal: 3m 7s\tremaining: 2.09s\n",
      "990:\tlearn: 0.1208108\ttotal: 3m 8s\tremaining: 1.71s\n",
      "992:\tlearn: 0.1207535\ttotal: 3m 8s\tremaining: 1.33s\n",
      "994:\tlearn: 0.1205700\ttotal: 3m 9s\tremaining: 950ms\n",
      "996:\tlearn: 0.1204626\ttotal: 3m 9s\tremaining: 570ms\n",
      "998:\tlearn: 0.1203908\ttotal: 3m 9s\tremaining: 190ms\n",
      "999:\tlearn: 0.1203166\ttotal: 3m 10s\tremaining: 0us\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.955067\n",
      "1  Precision  0.955089\n",
      "2     Recall  0.955119,      1    0    4    7    3    5    8    9    2    6\n",
      "1  808    0    3    1    2    3    5    0    4    1\n",
      "0    0  927    2    1    0    1    1    2    3    0\n",
      "4    2    2  789    8    9    1    2   11    9    2\n",
      "7    2    3   16  807    2   19    1    5    8    7\n",
      "3    1    2    4    0  774    0    5    2    6   20\n",
      "5    4    2    2    8    2  719   10    2    7    3\n",
      "8    3    2    1    0    0    6  812    0    3    0\n",
      "9    0    5    4    5    6    1    0  841    2   16\n",
      "2    4    6    4    9    0   12    4    1  762   11\n",
      "6    4    2    1    8   13    4    0   16    6  784)\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost_classifier = CatBoostClassifier(iterations=1000, depth=4, learning_rate=0.1, verbose=2)\n",
    "catboost_classifier.fit(X_train_norm, y_train)\n",
    "\n",
    "y_pred = catboost_classifier.predict(X_valid_norm)\n",
    "\n",
    "catboost_classifier_assessement = evaluate_classifier(y_valid, y_pred)\n",
    "print(catboost_classifier_assessement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 20:21:25.092408: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 105369600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7472 - loss: 0.8038 - val_accuracy: 0.9400 - val_loss: 0.2079\n",
      "Epoch 2/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9292 - loss: 0.2314 - val_accuracy: 0.9555 - val_loss: 0.1463\n",
      "Epoch 3/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.1625 - val_accuracy: 0.9620 - val_loss: 0.1266\n",
      "Epoch 4/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1356 - val_accuracy: 0.9646 - val_loss: 0.1170\n",
      "Epoch 5/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9650 - loss: 0.1113 - val_accuracy: 0.9689 - val_loss: 0.1046\n",
      "Epoch 6/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9714 - loss: 0.0942 - val_accuracy: 0.9692 - val_loss: 0.1033\n",
      "Epoch 7/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9741 - loss: 0.0840 - val_accuracy: 0.9706 - val_loss: 0.1025\n",
      "Epoch 8/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9759 - loss: 0.0752 - val_accuracy: 0.9714 - val_loss: 0.1021\n",
      "Epoch 9/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.0688 - val_accuracy: 0.9720 - val_loss: 0.0996\n",
      "Epoch 10/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9785 - loss: 0.0648 - val_accuracy: 0.9719 - val_loss: 0.1022\n",
      "Epoch 11/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0609 - val_accuracy: 0.9723 - val_loss: 0.0978\n",
      "Epoch 12/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9835 - loss: 0.0521 - val_accuracy: 0.9735 - val_loss: 0.1001\n",
      "Epoch 13/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0560 - val_accuracy: 0.9737 - val_loss: 0.1010\n",
      "Epoch 14/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9841 - loss: 0.0491 - val_accuracy: 0.9738 - val_loss: 0.0989\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.972242\n",
      "1  Precision  0.972277\n",
      "2     Recall  0.972262,      1    0    4    7    3    5    8    9    2    6\n",
      "1  812    0    2    2    1    1    6    0    2    1\n",
      "0    0  927    4    1    0    0    0    1    4    0\n",
      "4    4    0  813    4    4    0    1    4    4    1\n",
      "7    2    0   10  832    0   11    0    4    6    5\n",
      "3    0    1    0    0  799    0    3    0    0   11\n",
      "5    3    2    0    8    2  729    7    0    4    4\n",
      "8    2    0    0    0    0    5  818    0    2    0\n",
      "9    0    4    4    6    6    1    0  849    2    8\n",
      "2    4    6    2    0    0    5    4    1  786    5\n",
      "6    5    0    0    4   13    3    0    4    7  802)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_data, val_labels = self.validation_data\n",
    "        y_pred_proba = self.model.predict(val_data)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        f1 = f1_score(val_labels, y_pred, average='weighted')\n",
    "        print(f\"\\nEpoch {epoch+1} - F1 Score: {f1:.4f}\")\n",
    "        logs['f1_score'] = f1  # To store in logs if needed\n",
    "\n",
    "# Define the model\n",
    "mlp_classifier = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_norm.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_valid_norm, y_valid))\n",
    "\n",
    "# Train the model with the F1 callback\n",
    "mlp_classifier.fit(\n",
    "    X_train_norm, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_norm, y_valid),\n",
    "    callbacks=[early_stopping, f1_callback]\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_proba = mlp_classifier.predict(X_valid_norm)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Compute final F1 score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_valid, y_pred))\n",
    "\n",
    "mlp_classifier_assessment = evaluate_classifier(y_valid, np.round(y_pred))\n",
    "print(mlp_classifier_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "Train Loss: 0.2530, Train Accuracy: 0.9273, Train F1 Score: 0.9273\n",
      "Validation Loss: 0.0587, Validation Accuracy: 0.9808, Validation F1 Score: 0.9809\n",
      "\n",
      "Epoch 2/100\n",
      "Train Loss: 0.1173, Train Accuracy: 0.9659, Train F1 Score: 0.9659\n",
      "Validation Loss: 0.0492, Validation Accuracy: 0.9849, Validation F1 Score: 0.9849\n",
      "\n",
      "Epoch 3/100\n",
      "Train Loss: 0.1021, Train Accuracy: 0.9711, Train F1 Score: 0.9711\n",
      "Validation Loss: 0.0317, Validation Accuracy: 0.9902, Validation F1 Score: 0.9902\n",
      "\n",
      "Epoch 4/100\n",
      "Train Loss: 0.0806, Train Accuracy: 0.9770, Train F1 Score: 0.9770\n",
      "Validation Loss: 0.0326, Validation Accuracy: 0.9918, Validation F1 Score: 0.9918\n",
      "Early stopping counter: 1/3\n",
      "\n",
      "Epoch 5/100\n",
      "Train Loss: 0.0728, Train Accuracy: 0.9787, Train F1 Score: 0.9787\n",
      "Validation Loss: 0.0301, Validation Accuracy: 0.9913, Validation F1 Score: 0.9913\n",
      "\n",
      "Epoch 6/100\n",
      "Train Loss: 0.0664, Train Accuracy: 0.9805, Train F1 Score: 0.9805\n",
      "Validation Loss: 0.0514, Validation Accuracy: 0.9873, Validation F1 Score: 0.9873\n",
      "Early stopping counter: 1/3\n",
      "\n",
      "Epoch 7/100\n",
      "Train Loss: 0.0619, Train Accuracy: 0.9820, Train F1 Score: 0.9820\n",
      "Validation Loss: 0.0281, Validation Accuracy: 0.9918, Validation F1 Score: 0.9918\n",
      "\n",
      "Epoch 8/100\n",
      "Train Loss: 0.0534, Train Accuracy: 0.9849, Train F1 Score: 0.9849\n",
      "Validation Loss: 0.0260, Validation Accuracy: 0.9927, Validation F1 Score: 0.9927\n",
      "\n",
      "Epoch 9/100\n",
      "Train Loss: 0.0490, Train Accuracy: 0.9857, Train F1 Score: 0.9857\n",
      "Validation Loss: 0.0344, Validation Accuracy: 0.9911, Validation F1 Score: 0.9911\n",
      "Early stopping counter: 1/3\n",
      "\n",
      "Epoch 10/100\n",
      "Train Loss: 0.0461, Train Accuracy: 0.9868, Train F1 Score: 0.9868\n",
      "Validation Loss: 0.0340, Validation Accuracy: 0.9919, Validation F1 Score: 0.9919\n",
      "Early stopping counter: 2/3\n",
      "\n",
      "Epoch 11/100\n",
      "Train Loss: 0.0494, Train Accuracy: 0.9864, Train F1 Score: 0.9864\n",
      "Validation Loss: 0.0231, Validation Accuracy: 0.9937, Validation F1 Score: 0.9937\n",
      "\n",
      "Epoch 12/100\n",
      "Train Loss: 0.0408, Train Accuracy: 0.9885, Train F1 Score: 0.9885\n",
      "Validation Loss: 0.0341, Validation Accuracy: 0.9907, Validation F1 Score: 0.9907\n",
      "Early stopping counter: 1/3\n",
      "\n",
      "Epoch 13/100\n",
      "Train Loss: 0.0421, Train Accuracy: 0.9879, Train F1 Score: 0.9879\n",
      "Validation Loss: 0.0291, Validation Accuracy: 0.9929, Validation F1 Score: 0.9929\n",
      "Early stopping counter: 2/3\n",
      "\n",
      "Epoch 14/100\n",
      "Train Loss: 0.0381, Train Accuracy: 0.9893, Train F1 Score: 0.9893\n",
      "Validation Loss: 0.0267, Validation Accuracy: 0.9929, Validation F1 Score: 0.9929\n",
      "Early stopping counter: 3/3\n",
      "Early stopping triggered.\n",
      "Final Validation Accuracy: 0.9929\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       827\n",
      "           1       0.99      1.00      1.00       937\n",
      "           2       0.99      0.99      0.99       835\n",
      "           3       1.00      0.99      0.99       870\n",
      "           4       0.99      1.00      0.99       814\n",
      "           5       0.99      0.99      0.99       759\n",
      "           6       0.99      1.00      0.99       827\n",
      "           7       0.99      0.99      0.99       880\n",
      "           8       0.99      0.99      0.99       813\n",
      "           9       0.98      0.99      0.99       838\n",
      "\n",
      "    accuracy                           0.99      8400\n",
      "   macro avg       0.99      0.99      0.99      8400\n",
      "weighted avg       0.99      0.99      0.99      8400\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa66fd9450840e4806680b39c354c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1617d49f9aeb4e49af4eb67f0ab3d849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c934aa68ea473c8d5c65e3e6e96eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/307 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/yanncauchepin/kaggle_digitrecognizer_cnn_submission_df_2/commit/1b5be79d02dda63186a524ede508071ce1f3dcb0', commit_message='Upload dataset', commit_description='', oid='1b5be79d02dda63186a524ede508071ce1f3dcb0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/yanncauchepin/kaggle_digitrecognizer_cnn_submission_df_2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='yanncauchepin/kaggle_digitrecognizer_cnn_submission_df_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        \n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)), \n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),  \n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),  \n",
    "            \n",
    "            nn.Flatten(), \n",
    "            \n",
    "            nn.Linear(128 * 3 * 3, 100), \n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64, 10)  \n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "# Define transformation, dataset, and dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.reshape(28, 28).astype(np.float32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming X_train and X_valid are numpy arrays containing your data\n",
    "X_train_transformed = torch.stack([transform(x) for x in X_train]).to(device)\n",
    "X_valid_transformed = torch.stack([transform(x) for x in X_valid]).to(device)\n",
    "y_train_tensor = torch.tensor(y_train).to(device)\n",
    "y_valid_tensor = torch.tensor(y_valid).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_transformed, y_valid_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8)\n",
    "\n",
    "# Set up device, model, loss, and optimizer\n",
    "cnn_classifier = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(cnn_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training function with F1 score tracking\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    num_correct = 0\n",
    "    track_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(imgs)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        track_loss += loss.item()\n",
    "        \n",
    "        # Accumulate predictions and labels\n",
    "        all_preds.extend(torch.argmax(pred, dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Calculate metrics\n",
    "    epoch_loss = track_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Main training loop with early stopping\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, cnn_classifier, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation phase\n",
    "    cnn_classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = cnn_classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            all_val_preds.extend(torch.argmax(outputs, axis=1).cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(valid_loader)\n",
    "    val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average=\"weighted\")\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train F1 Score: {train_f1:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0  # Reset counter if validation loss improves\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stopping_counter}/{patience}\")\n",
    "    \n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Final Evaluation on validation set\n",
    "cnn_classifier.eval()\n",
    "all_cnn_classifier_preds, all_cnn_classifier_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = cnn_classifier(inputs)\n",
    "        preds = torch.argmax(outputs, axis=1)\n",
    "        \n",
    "        all_cnn_classifier_preds.extend(preds.cpu().numpy())\n",
    "        all_cnn_classifier_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate final accuracy and classification report\n",
    "accuracy = accuracy_score(all_cnn_classifier_labels, all_cnn_classifier_preds)\n",
    "print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(all_cnn_classifier_labels, all_cnn_classifier_preds))\n",
    "\n",
    "# Generate submission for test set\n",
    "X_test_transformed = torch.stack([transform(x) for x in X_test]).to(device)\n",
    "test_dataset = TensorDataset(X_test_transformed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "cnn_classifier.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0].to(device)\n",
    "        outputs = cnn_classifier(inputs)\n",
    "        preds = torch.argmax(outputs, axis=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "cnn_classifier_submission_df = pd.DataFrame({\n",
    "    \"ImageId\": range(1, len(test_preds) + 1),\n",
    "    \"Label\": test_preds\n",
    "})\n",
    "\n",
    "Dataset.from_pandas(cnn_classifier_submission_df).push_to_hub(\"yanncauchepin/kaggle_digitrecognizer_cnn_submission_df_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6006194cfa4672b4fedbcd5da80864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "custom_pytorch_cnn.pth:   0%|          | 0.00/898k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yanncauchepin/kaggle_digitrecognizer_cnn_model/commit/534026420a7ca029316f140f16fe2c97d7732f7e', commit_message='Upload custom_pytorch_cnn.py with huggingface_hub', commit_description='', oid='534026420a7ca029316f140f16fe2c97d7732f7e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yanncauchepin/kaggle_digitrecognizer_cnn_model', endpoint='https://huggingface.co', repo_type='model', repo_id='yanncauchepin/kaggle_digitrecognizer_cnn_model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "import json\n",
    "\n",
    "history_cnn = \"custom_pytorch_cnn\"\n",
    "os.makedirs(history_cnn, exist_ok=True)\n",
    "\n",
    "torch.save(cnn_classifier.state_dict(), os.path.join(history_cnn, \"custom_pytorch_cnn.pth\"))\n",
    "\n",
    "config = {\n",
    "    \"model_type\": \"custom_pytorch_cnn\",\n",
    "    \"num_labels\": 10,\n",
    "    \"input_size\": [1, 28, 28],\n",
    "    \"output_size\": 10\n",
    "}\n",
    "with open(f\"{history_cnn}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "    \n",
    "model_code = '''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        \n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)), \n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),  \n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),  \n",
    "            \n",
    "            nn.Flatten(), \n",
    "            \n",
    "            nn.Linear(128 * 3 * 3, 100), \n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64, 10)  \n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "'''\n",
    "with open(f\"{history_cnn}/custom_pytorch_cnn.py\", \"w\") as f:\n",
    "    f.write(model_code)\n",
    "    \n",
    "api = HfApi()\n",
    "repo_id = \"yanncauchepin/kaggle_digitrecognizer_cnn_model\"\n",
    "repo_url = api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/home/yanncauchepin/Git/Kaggle/DigitRecognizer/notebook/custom_pytorch_cnn/custom_pytorch_cnn.pth\",\n",
    "    path_in_repo=\"custom_pytorch_cnn.pth\",\n",
    "    repo_id=repo_id\n",
    ")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/home/yanncauchepin/Git/Kaggle/DigitRecognizer/notebook/custom_pytorch_cnn/config.json\",\n",
    "    path_in_repo=\"config.json\",\n",
    "    repo_id=repo_id\n",
    ")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/home/yanncauchepin/Git/Kaggle/DigitRecognizer/notebook/custom_pytorch_cnn/custom_pytorch_cnn.py\",\n",
    "    path_in_repo=\"custom_pytorch_cnn.py\",\n",
    "    repo_id=repo_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.5.1 (from torchvision)\n",
      "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yanncauchepin/Git/.venv/lib/python3.12/site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.5)\n",
      "Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m776.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:21\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0\n",
      "    Uninstalling torch-2.5.0:\n",
      "      Successfully uninstalled torch-2.5.0\n",
      "Successfully installed torch-2.5.1 torchvision-0.20.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Specify the absolute path for saving tensors\n",
    "external_path = Path(\"/media/yanncauchepin/ExternalDisk/RunningCode/digit_recognizer\")\n",
    "external_path.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(x.reshape(28, 28), dtype=torch.float32).unsqueeze(0)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Lambda(lambda x: x.expand(3, -1, -1)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Split and transform the dataset, saving each item to the external disk\n",
    "def save_data_to_disk(data, labels, transform, path, prefix=\"train\"):\n",
    "    for i, (img, label) in enumerate(zip(data, labels)):\n",
    "        img_tensor = transform(img)  # Apply transformations\n",
    "        torch.save((img_tensor, torch.tensor(label)), path / f\"{prefix}_{i}.pt\")  # Save to disk\n",
    "\n",
    "# Save the transformed training and validation data to external disk\n",
    "save_data_to_disk(X_train, y_train, transform, external_path, prefix=\"train\")\n",
    "save_data_to_disk(X_valid, y_valid, transform, external_path, prefix=\"valid\")\n",
    "\n",
    "# Custom Dataset to load data from external disk on-the-fly\n",
    "class DiskDataset(Dataset):\n",
    "    def __init__(self, path, prefix=\"train\"):\n",
    "        self.path = path\n",
    "        self.files = list(path.glob(f\"{prefix}_*.pt\"))  # Get all files with the prefix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx])  # Load tensor from disk\n",
    "        return data\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_dataset = DiskDataset(external_path, prefix=\"train\")\n",
    "valid_dataset = DiskDataset(external_path, prefix=\"valid\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 23:02:56.570102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-02 23:02:56.739895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-02 23:02:56.783626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 23:02:57.069157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-02 23:02:58.956709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanncauchepin/Git/.venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/11200 [00:00<?, ?it/s]/tmp/ipykernel_25585/2351370505.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.files[idx])  # Load tensor from disk\n",
      "100%|██████████| 11200/11200 [1:40:37<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.6977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [1:34:05<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [1:33:56<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training Loss: 0.1533\n",
      "Validation Accuracy: 0.9501\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       827\n",
      "           1       0.98      0.99      0.99       937\n",
      "           2       0.98      0.91      0.94       835\n",
      "           3       0.89      0.92      0.90       870\n",
      "           4       0.99      0.94      0.97       814\n",
      "           5       0.84      0.98      0.91       759\n",
      "           6       0.99      0.97      0.98       827\n",
      "           7       0.97      0.96      0.97       880\n",
      "           8       0.95      0.91      0.93       813\n",
      "           9       0.94      0.94      0.94       838\n",
      "\n",
      "    accuracy                           0.95      8400\n",
      "   macro avg       0.95      0.95      0.95      8400\n",
      "weighted avg       0.95      0.95      0.95      8400\n",
      "\n",
      "(      Metric     Value\n",
      "0   F1 Score  0.950554\n",
      "1  Precision  0.952667\n",
      "2     Recall  0.950119,      1    0    4    7    3    5    8    9    2    6\n",
      "1  799    0    1    1    0    7    1    0   18    0\n",
      "0    1  932    2    0    0    1    0    1    0    0\n",
      "4    2    7  756   45    1    8    4    5    6    1\n",
      "7    0    0    1  801    0   57    0    4    3    4\n",
      "3    2    2    2    2  767    1    5    7    1   25\n",
      "5    0    0    1    7    0  747    0    1    2    1\n",
      "8    1    1    1    0    0   21  801    0    2    0\n",
      "9    0    4    0    6    2    4    0  848    0   16\n",
      "2    2    1    4   31    0   27    0    2  740    6\n",
      "6    2    2    2    8    5   18    0    6    5  790)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a077e2c4456a47fb8ed18f10785a980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yanncauchepin/kaggle_digitrecognizer_vit_model/commit/00301fd101de7d31c7859b0db55ee00300b73253', commit_message='Upload ViTForImageClassification', commit_description='', oid='00301fd101de7d31c7859b0db55ee00300b73253', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yanncauchepin/kaggle_digitrecognizer_vit_model', endpoint='https://huggingface.co', repo_type='model', repo_id='yanncauchepin/kaggle_digitrecognizer_vit_model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize ViT model\n",
    "vit_classifier = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k', \n",
    "    num_labels=10\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit_classifier.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = AdamW(vit_classifier.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    vit_classifier.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = vit_classifier(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# Evaluation on validation set\n",
    "vit_classifier.eval()\n",
    "all_vit_classifier_preds, all_vit_classifier_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = vit_classifier(inputs)\n",
    "        preds = torch.argmax(outputs.logits, axis=1)\n",
    "        \n",
    "        all_vit_classifier_preds.extend(preds.cpu().numpy())\n",
    "        all_vit_classifier_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(all_vit_classifier_labels, all_vit_classifier_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(all_vit_classifier_labels, all_vit_classifier_preds))\n",
    "\n",
    "# Additional assessment (if function `evaluate_classifier` is defined)\n",
    "vit_classifier_assessment = evaluate_classifier(all_vit_classifier_labels, all_vit_classifier_preds)\n",
    "print(vit_classifier_assessment)\n",
    "\n",
    "vit_classifier.push_to_hub(\"yanncauchepin/kaggle_digitrecognizer_vit_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9334 [00:00<?, ?it/s]/tmp/ipykernel_25585/3633612418.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.files[idx])\n",
      "100%|██████████| 9334/9334 [34:53<00:00,  4.46it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'from_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 50\u001b[0m\n\u001b[1;32m     44\u001b[0m         test_preds\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     46\u001b[0m vit_classifier_submission_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageId\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_preds) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_preds\n\u001b[1;32m     49\u001b[0m })\n\u001b[0;32m---> 50\u001b[0m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m(vit_classifier_submission_df)\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myanncauchepin/kaggle_digitrecognizer_vit_submission_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'from_pandas'"
     ]
    }
   ],
   "source": [
    "def save_data_to_disk(data, labels, transform, path, prefix=\"train\"):\n",
    "    for i, (img, label) in enumerate(zip(data, labels)):\n",
    "        img_tensor = transform(img)  # Apply transformations\n",
    "        \n",
    "        # Save only the image tensor if the label is None\n",
    "        if label is None:\n",
    "            torch.save(img_tensor, path / f\"{prefix}_{i}.pt\")\n",
    "        else:\n",
    "            torch.save((img_tensor, torch.tensor(label)), path / f\"{prefix}_{i}.pt\")\n",
    "\n",
    "save_data_to_disk(X_test, [None] * len(X_test), transform, external_path, prefix=\"test\")  # Label is None for test\n",
    "\n",
    "class DiskDataset(Dataset):\n",
    "    def __init__(self, path, prefix=\"train\", include_labels=True):\n",
    "        self.path = path\n",
    "        self.files = list(path.glob(f\"{prefix}_*.pt\"))\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx])\n",
    "        \n",
    "        # Return data accordingly (without labels for test data)\n",
    "        if self.include_labels:\n",
    "            img_tensor, label = data\n",
    "            return img_tensor, label\n",
    "        else:\n",
    "            return data \n",
    "\n",
    "# Create a custom Dataset and DataLoader for the test set\n",
    "test_dataset = DiskDataset(external_path, prefix=\"test\", include_labels=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3)\n",
    "\n",
    "vit_classifier.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        inputs = batch.to(device)  \n",
    "        outputs = vit_classifier(inputs)\n",
    "        preds = torch.argmax(outputs.logits, axis=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "vit_classifier_submission_df = pd.DataFrame({\n",
    "    \"ImageId\": range(1, len(test_preds) + 1),\n",
    "    \"Label\": test_preds\n",
    "})\n",
    "from datasets import Dataset\n",
    "Dataset.from_pandas(vit_classifier_submission_df).push_to_hub(\"yanncauchepin/kaggle_digitrecognizer_vit_submission_df\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
